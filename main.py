# main.py
import time
import cv2
import numpy as np
import os

# ===============================
# ğŸ”¥ ê³µìœ  RUNNING í”Œë˜ê·¸
# ===============================
from modules.shared_flags import RUNNING

# ===============================
# ğŸ”¥ ë‹¨ì¼ ì¹´ë©”ë¼ ìŠ¤ë ˆë“œ
# ===============================
from modules.camera.camera_manager import start_camera_thread

# ===============================
# ğŸ”¥ ëª¨ë“ˆë³„ ìŠ¤ë ˆë“œ & í
# ===============================
from modules.pose.pose_thread_example import start_pose_thread, result_queue as pose_result_queue
from modules.gaze.gaze_thread_example import start_gaze_thread, gaze_result_queue
from modules.expression.expression_thread_example import start_expression_thread, expression_result_queue
from modules.hands.hand_thread_example import start_hands_thread, hands_result_queue
from modules.voice.voice_thread_example import start_voice_thread, voice_result_queue


# í‘œì • ëª¨ë“ˆì€ emotion_detector í•„ìš” ì—†ìŒ â†’ None ì‚¬ìš©
# GazeTrackerë¥¼ í‘œì •ì— ë„˜ê¸°ë©´ detect_faces ì—†ì–´ ì˜¤ë¥˜ë‚¨ (ì‚¬ìš© ê¸ˆì§€)


# ===============================
# ìµœì‹  ê°’ë§Œ ê°€ì ¸ì˜¤ê¸°
# ===============================
def drain_queue(q):
    latest = None
    while not q.empty():
        latest = q.get()
    return latest


# ===============================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ===============================
def main():
    # ğŸ”¥ í‘œì •ëª¨ë“ˆì€ ê°ì •ëª¨ë¸ì´ ì—†ìœ¼ë¯€ë¡œ None ì „ë‹¬
    emotion_detector = None

    # ğŸ”¥ ë°˜ë“œì‹œ ì´ ìˆœì„œë¡œ ì‹œì‘
    start_camera_thread()  # 1ê°œ ì¹´ë©”ë¼ë§Œ ê³µìœ 
    start_pose_thread()
    gaze_thread = start_gaze_thread()
    start_expression_thread(emotion_detector)  # â† ìˆ˜ì •ë¨
    start_hands_thread()
    start_voice_thread()

    print("\nğŸš€ AI Mock Interview â€” Main Started (q ë˜ëŠ” Xë¡œ ì¢…ë£Œ)\n")

    latest_pose = None
    latest_gaze = None
    latest_expr = None
    latest_hands = None
    latest_voice = None

    window_name = "AI Mock Interview - Dashboard"

    while True:

        # ===== ìµœì‹  ë°ì´í„° ìˆ˜ì‹  =====
        pose_data = drain_queue(pose_result_queue)
        gaze_data = drain_queue(gaze_result_queue)
        expr_data = drain_queue(expression_result_queue)
        hands_data = drain_queue(hands_result_queue)
        voice_data = drain_queue(voice_result_queue)

        if pose_data is not None:
            latest_pose = pose_data
        if gaze_data is not None:
            latest_gaze = gaze_data
        if expr_data is not None:
            latest_expr = expr_data
        if hands_data is not None:
            latest_hands = hands_data
        if voice_data is not None:
            latest_voice = voice_data

        # ============================
        # ëŒ€ì‹œë³´ë“œ í™”ë©´
        # ============================
        dashboard = np.zeros((800, 1200, 3), dtype=np.uint8)

        cv2.putText(
            dashboard,
            "AI Mock Interview Dashboard",
            (20, 40),
            cv2.FONT_HERSHEY_SIMPLEX,
            1.0,
            (0, 255, 0),
            2,
        )

        # ========== ìì„¸ =============
        if latest_pose is not None:
            frame, motion, _ = latest_pose
            f = cv2.resize(frame, (350, 250))
            dashboard[80:330, 20:370] = f
            cv2.putText(dashboard, f"Movement: {motion:.2f}",
                        (20, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        # ========== ì‹œì„  =============
        if latest_gaze is not None:
            frame, g = latest_gaze
            f = cv2.resize(frame, (350, 250))
            dashboard[80:330, 400:750] = f
            cv2.putText(dashboard, f"Gaze: {g['left_right']} / {g['up_down']}",
                        (400, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        # ========== ì† =============
        if isinstance(latest_hands, np.ndarray):
            f = cv2.resize(latest_hands, (350, 250))
            dashboard[350:600, 400:750] = f

        # ========== í‘œì • =============
        if latest_expr is not None and isinstance(latest_expr[1], dict):
            emo = latest_expr[1]
            cv2.putText(dashboard, f"Expression: {emo['dominant']}",
                        (20, 380), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        # ========== ìŒì„± =============
        if latest_voice is not None:
            text = latest_voice["text"]
            safe_text = text[:50] if text else "(ìŒì„± ì¸ì‹ ì‹¤íŒ¨)"

            cv2.putText(dashboard, f"Voice: {safe_text}",
                        (20, 430), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        # ============================
        # ì°½ í‘œì‹œ
        # ============================
        try:
            cv2.imshow(window_name, dashboard)
        except cv2.error:
            print("ğŸ”¥ imshow error â€” window closed")
            break

        # X ë‹«ê¸° í™•ì¸
        if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
            print("ğŸ”¥ Window closed by user.")
            break

        # q ì¢…ë£Œ
        key = cv2.waitKey(1) & 0xFF

        if key == ord('c'):
            from modules.gaze.gaze_thread_example import request_gaze_calibration
            request_gaze_calibration()
            print("âœ… 'c' pressed â†’ Gaze center calibration requested")

        if key == ord('q'):
            print("ğŸ”š 'q' pressed. Exiting.")
            breakgit remote -v

    # ============================
    # ğŸ”¥ ì „ì²´ ìŠ¤ë ˆë“œ ì¢…ë£Œ
    # ============================
    import modules.shared_flags as flags
    flags.RUNNING = False
    gaze_thread.join()
    cv2.destroyAllWindows()
    print("ğŸ§¹ Threads stopped.")

    return


# ============================
# ì‹¤í–‰ ì‹œì‘
# ============================
if __name__ == "__main__":
    main()
